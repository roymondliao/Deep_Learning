{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting ../../DeepLearning/TensorFlow/MNIST_data/train-images-idx3-ubyte.gz\n",
      "Extracting ../../DeepLearning/TensorFlow/MNIST_data/train-labels-idx1-ubyte.gz\n",
      "Extracting ../../DeepLearning/TensorFlow/MNIST_data/t10k-images-idx3-ubyte.gz\n",
      "Extracting ../../DeepLearning/TensorFlow/MNIST_data/t10k-labels-idx1-ubyte.gz\n"
     ]
    }
   ],
   "source": [
    "import tensorflow.examples.tutorials.mnist.input_data as input_data\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "\n",
    "# load mnist data\n",
    "mnist = input_data.read_data_sets(\"../../DeepLearning/TensorFlow/MNIST_data/\", one_hot=True)\n",
    "\n",
    "# define parameter \n",
    "learning_rate = 0.001\n",
    "training_iters = 50000\n",
    "batch_size = 128\n",
    "display_step = 10\n",
    "dropout = 0.75\n",
    "\n",
    "n_hidden_1 = 256\n",
    "n_hidden_2 = 256\n",
    "n_input = 784 # 28x28\n",
    "n_classes =10 # output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iter:1280, Minibatch Loss:49660.33203125, training accuracy:0.09375\n",
      "Iter:2560, Minibatch Loss:31652.541015625, training accuracy:0.25\n",
      "Iter:3840, Minibatch Loss:17414.33984375, training accuracy:0.3359375\n",
      "Iter:5120, Minibatch Loss:13953.4794921875, training accuracy:0.3984375\n",
      "Iter:6400, Minibatch Loss:9298.1953125, training accuracy:0.59375\n",
      "Iter:7680, Minibatch Loss:9277.0205078125, training accuracy:0.53125\n",
      "Iter:8960, Minibatch Loss:8109.361328125, training accuracy:0.5234375\n",
      "Iter:10240, Minibatch Loss:6327.27587890625, training accuracy:0.65625\n",
      "Iter:11520, Minibatch Loss:3052.865234375, training accuracy:0.75\n",
      "Iter:12800, Minibatch Loss:6686.3974609375, training accuracy:0.6875\n",
      "Iter:14080, Minibatch Loss:2769.619384765625, training accuracy:0.7109375\n",
      "Iter:15360, Minibatch Loss:3250.087890625, training accuracy:0.765625\n",
      "Iter:16640, Minibatch Loss:4017.419921875, training accuracy:0.78125\n",
      "Iter:17920, Minibatch Loss:1828.7677001953125, training accuracy:0.8515625\n",
      "Iter:19200, Minibatch Loss:2888.056640625, training accuracy:0.8125\n",
      "Iter:20480, Minibatch Loss:1940.570556640625, training accuracy:0.859375\n",
      "Iter:21760, Minibatch Loss:3855.76318359375, training accuracy:0.71875\n",
      "Iter:23040, Minibatch Loss:1069.5865478515625, training accuracy:0.8984375\n",
      "Iter:24320, Minibatch Loss:2981.441650390625, training accuracy:0.8125\n",
      "Iter:25600, Minibatch Loss:2446.947998046875, training accuracy:0.8515625\n",
      "Iter:26880, Minibatch Loss:1542.598876953125, training accuracy:0.859375\n",
      "Iter:28160, Minibatch Loss:1927.61279296875, training accuracy:0.859375\n",
      "Iter:29440, Minibatch Loss:2430.46923828125, training accuracy:0.859375\n",
      "Iter:30720, Minibatch Loss:1640.72265625, training accuracy:0.796875\n",
      "Iter:32000, Minibatch Loss:1310.47705078125, training accuracy:0.8828125\n",
      "Iter:33280, Minibatch Loss:1235.90576171875, training accuracy:0.8671875\n",
      "Iter:34560, Minibatch Loss:1222.771484375, training accuracy:0.8671875\n",
      "Iter:35840, Minibatch Loss:1270.1131591796875, training accuracy:0.890625\n",
      "Iter:37120, Minibatch Loss:1694.181640625, training accuracy:0.859375\n",
      "Iter:38400, Minibatch Loss:675.576416015625, training accuracy:0.96875\n",
      "Iter:39680, Minibatch Loss:601.3612060546875, training accuracy:0.9453125\n",
      "Iter:40960, Minibatch Loss:1609.78955078125, training accuracy:0.8671875\n",
      "Iter:42240, Minibatch Loss:1361.57275390625, training accuracy:0.8984375\n",
      "Iter:43520, Minibatch Loss:558.386474609375, training accuracy:0.9296875\n",
      "Iter:44800, Minibatch Loss:722.281982421875, training accuracy:0.90625\n",
      "Iter:46080, Minibatch Loss:1198.037109375, training accuracy:0.8984375\n",
      "Iter:47360, Minibatch Loss:1921.071044921875, training accuracy:0.8515625\n",
      "Iter:48640, Minibatch Loss:2497.82470703125, training accuracy:0.828125\n",
      "Iter:49920, Minibatch Loss:1878.7298583984375, training accuracy:0.84375\n",
      "Iter:51200, Minibatch Loss:1008.7826538085938, training accuracy:0.8984375\n",
      "Iter:52480, Minibatch Loss:409.76715087890625, training accuracy:0.9296875\n",
      "Iter:53760, Minibatch Loss:385.1256103515625, training accuracy:0.9609375\n",
      "Iter:55040, Minibatch Loss:1720.9681396484375, training accuracy:0.890625\n",
      "Iter:56320, Minibatch Loss:1491.741943359375, training accuracy:0.828125\n",
      "Iter:57600, Minibatch Loss:1666.8310546875, training accuracy:0.8984375\n",
      "Iter:58880, Minibatch Loss:1232.5660400390625, training accuracy:0.875\n",
      "Iter:60160, Minibatch Loss:1393.347900390625, training accuracy:0.8984375\n",
      "Iter:61440, Minibatch Loss:1237.2415771484375, training accuracy:0.859375\n",
      "Iter:62720, Minibatch Loss:726.687744140625, training accuracy:0.90625\n",
      "Iter:64000, Minibatch Loss:377.975341796875, training accuracy:0.9453125\n",
      "Iter:65280, Minibatch Loss:732.12060546875, training accuracy:0.890625\n",
      "Iter:66560, Minibatch Loss:1425.9996337890625, training accuracy:0.8984375\n",
      "Iter:67840, Minibatch Loss:455.1209716796875, training accuracy:0.9375\n",
      "Iter:69120, Minibatch Loss:805.6864013671875, training accuracy:0.8984375\n",
      "Iter:70400, Minibatch Loss:1011.719970703125, training accuracy:0.921875\n",
      "Iter:71680, Minibatch Loss:1865.98193359375, training accuracy:0.84375\n",
      "Iter:72960, Minibatch Loss:847.947509765625, training accuracy:0.90625\n",
      "Iter:74240, Minibatch Loss:988.0476684570312, training accuracy:0.90625\n",
      "Iter:75520, Minibatch Loss:628.7900390625, training accuracy:0.9296875\n",
      "Iter:76800, Minibatch Loss:373.5029296875, training accuracy:0.9296875\n",
      "Iter:78080, Minibatch Loss:711.5384521484375, training accuracy:0.9296875\n",
      "Iter:79360, Minibatch Loss:340.8941345214844, training accuracy:0.953125\n",
      "Iter:80640, Minibatch Loss:591.3150634765625, training accuracy:0.9296875\n",
      "Iter:81920, Minibatch Loss:1151.13232421875, training accuracy:0.890625\n",
      "Iter:83200, Minibatch Loss:1432.841552734375, training accuracy:0.875\n",
      "Iter:84480, Minibatch Loss:292.38238525390625, training accuracy:0.9609375\n",
      "Iter:85760, Minibatch Loss:678.67041015625, training accuracy:0.8984375\n",
      "Iter:87040, Minibatch Loss:270.8956298828125, training accuracy:0.9375\n",
      "Iter:88320, Minibatch Loss:685.7423095703125, training accuracy:0.90625\n",
      "Iter:89600, Minibatch Loss:788.7070922851562, training accuracy:0.9140625\n",
      "Iter:90880, Minibatch Loss:429.36883544921875, training accuracy:0.921875\n",
      "Iter:92160, Minibatch Loss:652.5504150390625, training accuracy:0.9140625\n",
      "Iter:93440, Minibatch Loss:534.5093994140625, training accuracy:0.8984375\n",
      "Iter:94720, Minibatch Loss:138.0648651123047, training accuracy:0.96875\n",
      "Iter:96000, Minibatch Loss:561.6212158203125, training accuracy:0.8828125\n",
      "Iter:97280, Minibatch Loss:433.8715515136719, training accuracy:0.953125\n",
      "Iter:98560, Minibatch Loss:1438.1361083984375, training accuracy:0.8828125\n",
      "Iter:99840, Minibatch Loss:725.23291015625, training accuracy:0.9296875\n",
      "Test accuracy:0.93359375\n"
     ]
    }
   ],
   "source": [
    "# define placeholder\n",
    "# shape of [-1] flattens into 1-D\n",
    "x = tf.placeholder(\"float\", [None, n_input])\n",
    "# construct data\n",
    "_X = tf.reshape(x, shape=[-1, 28, 28, 1]) \n",
    "y = tf.placeholder(\"float\", [None, n_classes])\n",
    "\n",
    "# first convolution layer - 32 features with 5x5 tensor\n",
    "wc1 = tf.Variable(tf.random_normal([5, 5, 1, 32]))\n",
    "bc1 = tf.Variable(tf.random_normal([32]))\n",
    "# Must have strides[0] = strides[3] = 1. \n",
    "# For the most common case of the same horizontal and vertices strides, \n",
    "# strides = [1, stride, stride, 1].\n",
    "# padding = \"SAME\" 是指說input layer與第一層的卷積結果是相同維度的，ex:input=28x28 conv1=28x28xfeatures\n",
    "conv1 = tf.nn.relu(tf.nn.bias_add(tf.nn.conv2d(_X, filter=wc1, strides=[1, 1, 1, 1], padding='SAME'), bc1))\n",
    "\n",
    "# max pooling\n",
    "k = 2\n",
    "conv1 = tf.nn.max_pool(conv1, ksize=[1, k, k, 1], strides=[1, k, k, 1], padding='SAME')\n",
    "\n",
    "# dropout\n",
    "keep_prob = tf.placeholder(tf.float32)\n",
    "conv1 = tf.nn.dropout(conv1, keep_prob)\n",
    "\n",
    "# second convolution layer\n",
    "wc2 = tf.Variable(tf.random_normal([5, 5, 32, 64]))\n",
    "bc2 = tf.Variable(tf.random_normal([64]))\n",
    "conv2 = tf.nn.relu(tf.nn.bias_add(tf.nn.conv2d(conv1, filter=wc2, strides=[1, 1, 1, 1], padding='SAME'), bc2))\n",
    "k = 2\n",
    "conv2 = tf.nn.max_pool(conv2, ksize=[1, k, k, 1], strides=[1, k, k, 1], padding='SAME')\n",
    "conv2 = tf.nn.dropout(conv2, keep_prob)\n",
    "\n",
    "# flatten \n",
    "wd1 = tf.Variable(tf.random_normal([7*7*64, 1024]))\n",
    "bd1 = tf.Variable(tf.random_normal([1024]))\n",
    "dense1 = tf.reshape(conv2, [-1, wd1.get_shape().as_list()[0]])\n",
    "dense1 = tf.nn.relu(tf.add(tf.matmul(dense1, wd1), bd1))\n",
    "dense1 = tf.nn.dropout(dense1, keep_prob)\n",
    "\n",
    "# fully connection layer\n",
    "wout = tf.Variable(tf.random_normal([1024, n_classes]))\n",
    "bout = tf.Variable(tf.random_normal([n_classes]))\n",
    "pred = tf.add(tf.matmul(dense1, wout), bout)\n",
    "\n",
    "# cost function & optimizer\n",
    "cost = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(pred, y))\n",
    "optimizer = tf.train.AdamOptimizer(learning_rate=learning_rate).minimize(cost)\n",
    "correct_pred = tf.equal(tf.argmax(pred, 1), tf.argmax(y, 1))\n",
    "accuracy = tf.reduce_mean(tf.cast(correct_pred, tf.float32))\n",
    "\n",
    "# run\n",
    "init = tf.global_variables_initializer()\n",
    "with tf.Session() as sess:\n",
    "    sess.run(init)\n",
    "    step = 1\n",
    "    while step * batch_size < training_iters:\n",
    "        batch_xs, batch_ys = mnist.train.next_batch(batch_size)\n",
    "        sess.run(optimizer, feed_dict={x:batch_xs, y:batch_ys, keep_prob: dropout})\n",
    "        if step % display_step ==0:\n",
    "            acc = sess.run(accuracy, feed_dict={x:batch_xs, y:batch_ys, keep_prob: 1.})\n",
    "            loss = sess.run(cost, feed_dict={x:batch_xs, y:batch_ys, keep_prob: 1.})\n",
    "            print(\"Iter:{}, Minibatch Loss:{}, training accuracy:{}\".format(str(step*batch_size),\n",
    "                                                                           loss, acc))\n",
    "        step += 1\n",
    "    print(\"Test accuracy:{}\".format(sess.run(accuracy, feed_dict={x:mnist.test.images[:256], \n",
    "                                                                  y:mnist.test.labels[:256], \n",
    "                                                                  keep_prob: 1.})))\n",
    "print(\"Finished!\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
